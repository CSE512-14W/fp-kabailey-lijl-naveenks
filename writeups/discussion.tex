
\subsection{Instructor Feedback}
We started the design with the goals of a single professor in mind. He was most interested in a few factors, which we sought to incorporate in our design:
\begin{enumerate}
\item{} Drop-off rate in student retention: how much is normal attrition and how much is caused by the course?
\item{} Cross-offering comparisons: was there significant difference between offerings?
\item{} Performance of female students in the course overall (programming languages is a male-dominated course, in large part. Back-of-the envelope calculations showed that his retention was worse for the female students: was there a reason why?
\end{enumerate}
Though we targeted all three of these questions, we were most successful at answering the first two. Though we allow the instructors to explore trends within demographic groups, the depth of options in exploring the "why" of student retention coupled with demographic variables is not expressed. Further insight into this direction would be possible with stacked filters or pop-up boxes on hover that provide dominant information for a cluster or individual student highlighted. However, this is beyond the scope of the current implementation. 

\subsection{Student Feedback}
Though we initially targeted our design at the instructors of MOOC courses, we found that students 
expressed an equal amount of interest in the data. Both our classmates and other graduate students
who heard about the project were eager to discover whether similar visualizations, perhaps better 
aggregated, could be used to isolate the \emph{difficult weeks} of a course, or which assignments and 
exam questions they were most likely to struggle on. 

\subsection{Additional Feedback}
In presenting an early prototype of our project to a wider community base, via poster, we received plenty of additional feedback. Students and instructors were both intrigued by the retention rate and wanted to ask many more questions about the reasons students left. In addition, many professors were intrigued by the idea of isolating "problem" sections of their exams where they might improve, which drove the development of further exam views. 

Our implementation of a timescale view was driven entirely by responses to the initial prototype, in which a professor confessed she'd always written down the times that assignments and exams were turned in, and had never bothered to plot them to find out if students turned in exams early because they were clueless or brilliant. 

Additional views and data collection schemes were encouraged by nearly everyone, showing that the quantity  and diversity of questions that can be asked in this arena is quite startling. 

Finally, several people at the university level have  expressed interest in incorporating a system such as this into their annual course analytics, incorporating student evaluations of the course and professor's teaching into the mix, but analyzing pure classroom data.